{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAaPfm0OasbUIkMZNTU2jJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexdriedger/MTGDataExperiments/blob/main/build_around_evaluator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Use This Notebook\n",
        "\n",
        "## Show Me The Cool Graphs\n",
        "\n",
        "If you want to see the graphs for build arounds and explore the data for various build around strategies, hop over to the [Examples](#Examples) section. There is more info there on how to see some example build arounds (eg. Caves from LCI, Roots from MKM) and how to customize the evaluation (even if you don't have any coding experience).\n",
        "\n",
        "## Show Me The Code\n",
        "\n",
        "If you are interested in how the graphs are created or want to tweak the code to test out different things, all the code is in the [Code](#Code) section\n",
        "\n",
        "## How to Run This Notebook\n",
        "\n",
        "To run this notebook, in the menu, click \"Runtime\" => \"Run all\". It usually takes a few minutes to process all the data. You can see the graphs in the [Examples](#Examples) secion"
      ],
      "metadata": {
        "id": "GUyMRcJn0P0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n",
        "\n",
        "This section has all the code for creating the datasets and graphing the build arounds. If you want to just see the graphs, go to [Examples](#Examples)."
      ],
      "metadata": {
        "id": "0-QyUCpm3B3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Pandas and Set Useful Options\n",
        "\n",
        "Pandas is what we'll use to analyze the data. For more info on Pandas, see the [docs](https://pandas.pydata.org/docs/user_guide/10min.html)."
      ],
      "metadata": {
        "id": "CtQPSk8Mn1V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_columns', 1600)"
      ],
      "metadata": {
        "id": "9NJusHEvH1xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the Dataset\n",
        "\n",
        "Download the datasets. The datasets are from the [17 Lands Public Data Sets](https://www.17lands.com/public_datasets)."
      ],
      "metadata": {
        "id": "UFKSSYrABRPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "FORMAT_PREMIER_DRAFT = \"PremierDraft\"\n",
        "FORMAT_TRADITIONAL_DRAFT = \"TradDraft\"\n",
        "\n",
        "def download_if_not_exists(file_name, remote_url, common_name):\n",
        "  if Path(file_name).is_file():\n",
        "    print(f\"{common_name} data file already downloaded. Skipping download\")\n",
        "  else:\n",
        "    !wget {remote_url}\n",
        "\n",
        "def get_dataset_metadata(expansion, format, dataset_type):\n",
        "  file_name = f\"{dataset_type}_data_public.{expansion}.{format}.csv.gz\"\n",
        "  remote_url = f\"https://17lands-public.s3.amazonaws.com/analysis_data/{dataset_type}_data/{file_name}\"\n",
        "  return file_name, remote_url\n",
        "\n",
        "\n",
        "def download_datasets(expansion, format):\n",
        "  game_data_file_name, game_data_remote_url = get_dataset_metadata(expansion, format, \"game\")\n",
        "  draft_data_file_name, draft_data_remote_url = get_dataset_metadata(expansion, format, \"draft\")\n",
        "  cards_data_file_name = f\"cards.csv\"\n",
        "  cards_data_remote_url = f\"https://17lands-public.s3.amazonaws.com/analysis_data/cards/{cards_data_file_name}\"\n",
        "\n",
        "  download_if_not_exists(game_data_file_name, game_data_remote_url, \"game\")\n",
        "  download_if_not_exists(draft_data_file_name, draft_data_remote_url, \"draft\")\n",
        "  download_if_not_exists(cards_data_file_name, cards_data_remote_url, \"cards\")"
      ],
      "metadata": {
        "id": "0VqyFmXznDeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enrich the dataset\n",
        "\n",
        "Iterate through the dataset in chunks and add counts for build around enablers and payoffs. We drop all the specific card information after calculating the counts in order to save space."
      ],
      "metadata": {
        "id": "vl4xFbz7qPuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_drafts(expansion, format):\n",
        "  \"\"\"\n",
        "  Returns a table with one row per draft\n",
        "  \"\"\"\n",
        "  cols = [\"expansion\", \"event_type\", \"draft_id\", \"draft_time\", \"rank\", \"event_match_wins\", \"event_match_losses\"]\n",
        "  chunks = list()\n",
        "  for draft_data in pd.read_csv(\n",
        "      get_dataset_metadata(expansion, format, \"draft\")[0],\n",
        "      chunksize=100000,\n",
        "      usecols=cols\n",
        "      ):\n",
        "    draft_data_no_dups = draft_data.drop_duplicates(subset=[\"draft_id\"])\n",
        "    chunks.append(draft_data_no_dups)\n",
        "\n",
        "  all_drafts = pd.concat(chunks)\n",
        "\n",
        "  # Remove duplicates in case of drafts that show up in multiple chunks\n",
        "  all_drafts = all_drafts.drop_duplicates(subset=[\"draft_id\"], keep=\"last\")\n",
        "  return all_drafts"
      ],
      "metadata": {
        "id": "DNTOjw4gqPH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def enrich_build_around_stats(decks, build_around_payoffs, build_around_enablers):\n",
        "  decks[\"build_around_payoffs\"] = decks[build_around_payoffs].sum(axis=1)\n",
        "  decks[\"build_around_enablers\"] = decks[build_around_enablers].sum(axis=1)\n",
        "\n",
        "  return decks\n",
        "\n",
        "gd_base_cols = ['draft_id', 'main_colors', 'splash_colors', 'rank', 'on_play', 'opp_colors', 'num_turns', 'draft_time', 'won']\n",
        "def get_game_data_cols(format, expansion):\n",
        "  \"\"\"\n",
        "  Returns the columns in the game data file that includes metadata and which cards were in the deck. Filters out other columns to reduce size of the dataset\n",
        "  \"\"\"\n",
        "  df = next(pd.read_csv(get_dataset_metadata(format, expansion, \"game\")[0], chunksize=100))\n",
        "  col_names = list(df)\n",
        "  gd_card_cols = [x for x in col_names if x.startswith(\"deck_\")]\n",
        "\n",
        "  gd_all_cols = gd_base_cols + gd_card_cols\n",
        "  return gd_all_cols\n",
        "\n",
        "def get_all_enriched_decks(\n",
        "    expansion,\n",
        "    format,\n",
        "    build_around_payoffs,\n",
        "    build_around_enablers,\n",
        "\n",
        "    # Additional filters\n",
        "    start_date: datetime = None,\n",
        "    end_date: datetime = None,\n",
        "    ranks: list[str] = None,\n",
        "    on_play: bool = None,\n",
        "    min_num_turns: int = None,\n",
        "    max_num_turns: int = None,\n",
        "    main_colors: list[str] = None\n",
        "    ):\n",
        "  gd_all_cols = get_game_data_cols(expansion, format)\n",
        "  final_cols = gd_base_cols + [\"build_around_payoffs\", \"build_around_enablers\"]\n",
        "  chunks = list()\n",
        "  for game_data in pd.read_csv(\n",
        "      get_dataset_metadata(expansion, format, \"game\")[0],\n",
        "      chunksize=100000,\n",
        "      usecols=gd_all_cols\n",
        "      ):\n",
        "    game_data = game_data.rename(columns=lambda x: x[5:] if x.startswith(\"deck_\") else x)\n",
        "    game_data['draft_time'] = pd.to_datetime(game_data['draft_time'])\n",
        "    game_data = enrich_build_around_stats(game_data, build_around_payoffs, build_around_enablers)\n",
        "    # game_data = game_data.filter(items=final_cols)\n",
        "\n",
        "    # Additional optional filters\n",
        "    if start_date:\n",
        "      game_data = game_data[game_data[\"draft_time\"] >= start_date]\n",
        "    if end_date:\n",
        "      game_data = game_data[game_data[\"draft_time\"] <= end_date]\n",
        "    if ranks:\n",
        "      game_data = game_data[game_data[\"rank\"].isin(ranks)]\n",
        "    if on_play:\n",
        "      game_data = game_data[game_data[\"on_play\"] == on_play]\n",
        "    if min_num_turns:\n",
        "      game_data = game_data[game_data[\"num_turns\"] >= min_num_turns]\n",
        "    if max_num_turns:\n",
        "      game_data = game_data[game_data[\"num_turns\"] <= max_num_turns]\n",
        "    if main_colors:\n",
        "      game_data = game_data[game_data[\"main_colors\"].isin(main_colors)]\n",
        "\n",
        "    chunks.append(game_data)\n",
        "\n",
        "  all_games = pd.concat(chunks)\n",
        "  return all_games"
      ],
      "metadata": {
        "id": "xeK_um_uiTOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregate the Dataset\n",
        "\n",
        "The aggregation gets the win percentage and size for every pair of payoffs count to enablers count (eg. what was the win percentage with 3 payoffs and 7 enablers)"
      ],
      "metadata": {
        "id": "obXs1RDUsYsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_build_around_stats(decks):\n",
        "  win_col_name = \"won_int\"\n",
        "  df = decks.copy()\n",
        "\n",
        "  df[\"won_int\"] = df[\"won\"].astype(int)\n",
        "\n",
        "  df = df.filter(items=[\"build_around_payoffs\", \"build_around_enablers\", win_col_name])\n",
        "  df = df.groupby([\"build_around_payoffs\", \"build_around_enablers\"]).agg([\"mean\", \"size\"])\n",
        "\n",
        "  df = df[win_col_name].copy()\n",
        "  df = df.reset_index()\n",
        "  return df"
      ],
      "metadata": {
        "id": "eRtkNQp--n1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph the Dataset\n",
        "\n",
        "Now that the data has been enriched and aggregated, we can graph the data with a heatmap. Note, we only show boxes that have enough data points to be relevant"
      ],
      "metadata": {
        "id": "aMS30NpNOlEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "G75hJ0WJPM35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_build_around_data(build_around_decks, title=\"Build Around Payoffs\"):\n",
        "  max_payoffs = build_around_decks[\"build_around_payoffs\"].max() + 1\n",
        "  max_enablers = build_around_decks[\"build_around_enablers\"].max() + 1\n",
        "\n",
        "  hm = np.zeros((max_payoffs, max_enablers))\n",
        "  hm.fill(None)\n",
        "\n",
        "  for index, row in build_around_decks.iterrows():\n",
        "    non_flip_cave_cards = int(row[\"build_around_enablers\"])\n",
        "    cave_payoffs = int(row[\"build_around_payoffs\"])\n",
        "    mean = row[\"mean\"]\n",
        "    hm[cave_payoffs, non_flip_cave_cards] = mean\n",
        "\n",
        "  hm_df = DataFrame(hm, index=range(max_payoffs), columns=range(max_enablers))\n",
        "  hm_df = hm_df[::-1]\n",
        "\n",
        "  plt.figure(figsize=(max_enablers, max_payoffs))\n",
        "\n",
        "  vmin = hm[0, 0] - .1\n",
        "  vmax = hm[0, 0] + 0.04\n",
        "  center = hm[0, 0] - 0.03\n",
        "\n",
        "  sns.set(style=\"ticks\")\n",
        "  plt.style.use(\"dark_background\")\n",
        "\n",
        "  ax = sns.heatmap(\n",
        "      hm_df,\n",
        "      annot=True,\n",
        "      cmap=\"RdYlGn\",\n",
        "      robust=True,\n",
        "      fmt=\".2f\",\n",
        "      linewidth=.25,\n",
        "      linecolor=\"black\",\n",
        "      center=center,\n",
        "      vmin=vmin,\n",
        "      vmax=vmax,\n",
        "      cbar=False\n",
        "      )\n",
        "  ax.xaxis.labelpad = 20\n",
        "  ax.yaxis.labelpad = 20\n",
        "\n",
        "  ax.set_xlabel(\"Enablers in Deck\", fontsize=18)\n",
        "  ax.set_ylabel(\"Payoffs in Deck\", fontsize=18)\n",
        "\n",
        "  ax.set_title(title, fontdict={'fontsize': 28, 'fontweight': 3}, pad=20)\n",
        "  return ax"
      ],
      "metadata": {
        "id": "HfeWJO1EEZNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bring It All Together"
      ],
      "metadata": {
        "id": "sNby99uyQskF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_build_around(\n",
        "    title: str,\n",
        "    expansion: str,\n",
        "    format: str,\n",
        "    build_around_payoffs: list[str],\n",
        "    build_around_enablers: list[str],\n",
        "    num_games_threshold: int = 50,\n",
        "\n",
        "    # Additional filters\n",
        "    start_date: datetime = None,\n",
        "    end_date: datetime = None,\n",
        "    ranks: list[str] = None,\n",
        "    on_play: bool = None,\n",
        "    min_num_turns: int = None,\n",
        "    max_num_turns: int = None,\n",
        "    main_colors: list[str] = None\n",
        "    ):\n",
        "\n",
        "  download_datasets(expansion, format)\n",
        "  all_decks_with_build_around_data = get_all_enriched_decks(\n",
        "      expansion,\n",
        "      format,\n",
        "      build_around_payoffs,\n",
        "      build_around_enablers,\n",
        "      start_date,\n",
        "      end_date,\n",
        "      ranks,\n",
        "      on_play,\n",
        "      min_num_turns,\n",
        "      max_num_turns,\n",
        "      main_colors\n",
        "      )\n",
        "\n",
        "  aggregated_stats = aggregate_build_around_stats(all_decks_with_build_around_data)\n",
        "  aggregated_stats_with_thresh = aggregated_stats.drop(aggregated_stats[aggregated_stats[\"size\"] < num_games_threshold].index)\n",
        "  ax = graph_build_around_data(aggregated_stats_with_thresh, title)\n",
        "  ax\n",
        "  return all_decks_with_build_around_data, aggregated_stats, ax"
      ],
      "metadata": {
        "id": "16mNeiiCQF0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Stats\n",
        "\n",
        "These functions help evaluate the data in more detail"
      ],
      "metadata": {
        "id": "yphkBlys_OlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def common_cards(input_decks, payoff_min, payoff_max, build_around_min, build_around_max):\n",
        "  cols_to_exclude = gd_base_cols + [\"build_around_enablers\", \"build_around_payoffs\"]\n",
        "  decks_without_metadata_cols = input_decks[input_decks.columns[~input_decks.columns.isin(cols_to_exclude)]]\n",
        "  cc_stats = decks_without_metadata_cols.mean().sort_values(ascending=False)\n",
        "  print(\"Most common cards in decks (avg)\")\n",
        "  print(cc_stats.head(35))\n",
        "\n",
        "def color_breakdown(input_decks):\n",
        "  print(\"\\nBreakdown of decks by color groups\")\n",
        "  print(input_decks[\"main_colors\"].value_counts(normalize=True).sort_values(ascending=False).mul(100).astype(str)+'%')\n",
        "\n",
        "def extra_stats(input_decks, payoff_min, payoff_max, build_around_min, build_around_max):\n",
        "  optimal_decks = input_decks[(input_decks[\"build_around_payoffs\"] >= payoff_min) & (input_decks[\"build_around_payoffs\"] <= payoff_max) & (input_decks[\"build_around_enablers\"] >= build_around_min) & (input_decks[\"build_around_enablers\"] <= build_around_max)]\n",
        "  print(f\"\\nNumber of data points: {len(optimal_decks)}\")\n",
        "  common_cards(optimal_decks, payoff_min, payoff_max, build_around_min, build_around_max)\n",
        "  color_breakdown(optimal_decks)"
      ],
      "metadata": {
        "id": "7T4rDq_M_OlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Examples\"></a>\n",
        "# Examples\n",
        "\n",
        "To create the graphs, in the menu, click \"Runtime\" => \"Run all\". It usually takes a few minutes to process all the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "yeltuFtg0iuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MKM - Insidious Roots / Chalk Outline"
      ],
      "metadata": {
        "id": "5jr5VyW08mHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roots_outline_payoffs = [\"Insidious Roots\", \"Chalk Outline\"]\n",
        "top_enablers_by_stats = [\"Rubblebelt Maverick\", \"Gravestone Strider\", \"Vitu-Ghazi Inspector\", \"Topiary Panther\", \"Extract a Confession\", \"Bite Down on Crime\", \"Evidence Examiner\", \"Aftermath Analyst\", \"Leering Onlooker\"]\n",
        "\n",
        "# Other potential groups of enablers\n",
        "enablers_creatures_that_leave_the_graveyard = [\"Rubblebelt Maverick\", \"Gravestone Strider\", \"Leering Onlooker\"]\n",
        "collect_evidence_cards = [\"Behind the Mask\", \"Bite Down on Crime\", \"Crimestopper Sprite\", \"Evidence Examiner\", \"Extract a Confession\", \"Forensic Researcher\", \"Hedge Whisperer\", \"Polygraph Orb\", \"Sample Collector\", \"Surveillance Monitor\", \"Vitu-Ghazi Inspector\"]\n",
        "enablers_leave_the_graveyard = [\"Macabre Reconstruction\", \"Rot Farm Mortipede\", \"Soul Enervation\"]\n",
        "\n",
        "roots_outline_decks_premier, aggs, ax = evaluate_build_around(\n",
        "    title=\"Insidious Roots & Chalk Outline \\nPremier Draft\",\n",
        "    expansion=\"MKM\",\n",
        "    format=FORMAT_PREMIER_DRAFT,\n",
        "    build_around_payoffs=roots_outline_payoffs,\n",
        "    build_around_enablers=top_enablers_by_stats,\n",
        "    start_date=datetime(2024, 3, 1),\n",
        "    ranks=[\"platinum\", \"diamond\", \"mythic\"]\n",
        ")"
      ],
      "metadata": {
        "id": "AmJyp72c8mHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_stats(roots_outline_decks_premier, payoff_min=1, payoff_max=4, build_around_min=7, build_around_max=15)"
      ],
      "metadata": {
        "id": "EqfkWxA0BgPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LCI - Caves Deck"
      ],
      "metadata": {
        "id": "7JfoG1x48e7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cave_build_around_payoffs = [\"Bat Colony\", \"Calamitous Cave-In\", \"Gargantuan Leech\", \"Sinuous Benthisaur\"]\n",
        "cave_build_around_enablers = [\"Captivating Cave\", \"Cavernous Maw\", \"Echoing Deeps\", \"Forgotten Monument\", \"Hidden Cataract\", \"Hidden Courtyard\", \"Hidden Necropolis\", \"Hidden Nursery\", \"Hidden Volcano\", \"Pit of Offerings\", \"Promising Vein\", \"Sunken Citadel\", \"Volatile Fault\"]\n",
        "\n",
        "caves_deck, aggs, ax = evaluate_build_around(\n",
        "    title=\"Caves Decks\",\n",
        "    expansion=\"LCI\",\n",
        "    format=FORMAT_PREMIER_DRAFT,\n",
        "    build_around_payoffs=cave_build_around_payoffs,\n",
        "    build_around_enablers=cave_build_around_enablers,\n",
        "    ranks=[\"platinum\", \"diamond\", \"mythic\"]\n",
        ")"
      ],
      "metadata": {
        "id": "-aEDe9BAxy64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Stuff"
      ],
      "metadata": {
        "id": "eMX5pIbd0pXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore The Data Set\n",
        "\n",
        "There are 3 key datasets from 17Lands. The main datasets are the draft and game data sets for an MTG set. Another dataset that can often be useful is the list of all cards on arena.\n",
        "\n",
        "These are helpful to looking at the raw datasets"
      ],
      "metadata": {
        "id": "tbROPQjKHqCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Game Data\n",
        "df = next(pd.read_csv(get_dataset_metadata(\"LCI\", FORMAT_PREMIER_DRAFT, \"game\")[0], chunksize=100))\n",
        "df.head(25)"
      ],
      "metadata": {
        "id": "vIg5P6VaHvF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draft Data\n",
        "df = next(pd.read_csv(get_dataset_metadata(\"LCI\", FORMAT_PREMIER_DRAFT, \"draft\")[0], chunksize=100))\n",
        "df.head(55)"
      ],
      "metadata": {
        "id": "7hSoUD0BH2HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cards_table = pd.read_csv(\"cards.csv\")\n",
        "set_cards = cards_table[cards_table[\"expansion\"] == \"MKM\"]\n",
        "set_cards.head(25)"
      ],
      "metadata": {
        "id": "CxhCYVpEnTRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}